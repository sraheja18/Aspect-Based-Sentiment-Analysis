{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.bert import bert_ATE, bert_ABSA\n",
    "from data.dataset import dataset_ATM, dataset_ABSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n",
    "pretrain_model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrain_model_name)\n",
    "lr = 2e-5\n",
    "model_ATE = bert_ATE(pretrain_model_name).to(DEVICE)\n",
    "optimizer_ATE = torch.optim.Adam(model_ATE.parameters(), lr=lr)\n",
    "model_ABSA = bert_ABSA(pretrain_model_name).to(DEVICE)\n",
    "optimizer_ABSA = torch.optim.Adam(model_ABSA.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evl_time(t):\n",
    "    min, sec= divmod(t, 60)\n",
    "    hr, min = divmod(min, 60)\n",
    "    return int(hr), int(min), int(sec)\n",
    "\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path), strict=False)\n",
    "    return model\n",
    "    \n",
    "def save_model(model, name):\n",
    "    torch.save(model.state_dict(), name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/laptops_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenizer.tokenize(\"Hello how are you.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7592, 2129, 2024, 2017, 1012]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 2129, 2024, 2017, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello how are you.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acpect Term Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops_train_ds = dataset_ATM(pd.read_csv(\"data/laptops_train.csv\"), tokenizer)\n",
    "laptops_test_ds = dataset_ATM(pd.read_csv(\"data/laptops_test.csv\"), tokenizer)\n",
    "restaurants_train_ds = dataset_ATM(pd.read_csv(\"data/restaurants_train.csv\"), tokenizer)\n",
    "restaurants_test_ds = dataset_ATM(pd.read_csv(\"data/restaurants_test.csv\"), tokenizer)\n",
    "twitter_train_ds = dataset_ATM(pd.read_csv(\"data/twitter_train.csv\"), tokenizer)\n",
    "twitter_test_ds = dataset_ATM(pd.read_csv(\"data/twitter_test.csv\"), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ConcatDataset([laptops_train_ds, restaurants_train_ds, twitter_train_ds])\n",
    "test_ds = ConcatDataset([laptops_test_ds, restaurants_test_ds, twitter_test_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True)\n",
    "\n",
    "    pols_tensors = [s[3] for s in samples]\n",
    "    pols_tensors = pad_sequence(pols_tensors, batch_first=True)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, pols_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=5, collate_fn=create_mini_batch, shuffle = True)\n",
    "test_loader = DataLoader(test_ds, batch_size=50, collate_fn=create_mini_batch, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2096,  1045,  3262,  2224,  2009,  2005, 10373,  1010,  4274,  1998,\n",
      "         10355,  1010,  1045,  1000,  1049,  1000,  9657,  2035,  2060,  5097,\n",
      "          2444,  2039,  2000,  1996,  2152,  3115,  1045,  1000,  2310,  1000,\n",
      "          2272,  2000,  9120,  2013,  6097, 12191,  2015,  1012,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [ 1045,  2165,  2009,  2000,  2767,  2040,  8184,  4964,  2009,  1998,\n",
      "          1045,  2633,  3825,  2055, 10347,  2005,  1996,  3668, 10943,  2100,\n",
      "          1012,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [ 3492,  2172,  2589,  2026, 19453,  1012,  2085,  1045,  2031,  1017,\n",
      "          2847,  1998,  2570,  2781,  2000,  3191,  4302, 10693,  1012,  1024,\n",
      "          1011, 25269,  2497,  1011,  3398,  1010,  1045,  1000,  1049,  1000,\n",
      "          1037, 11265,  4103,  1012,  3066,  2007,  2009,  1012,  8840,  2140,\n",
      "          1012,     0,     0],\n",
      "        [ 1032,  1032,  1045,  5223,  1996,  3645,  1021,  1000,  1000,  2026,\n",
      "          2801,  1000,  1000,  4748, 16874,  2015,  1012,  1012,  1012,  9686,\n",
      "          2361,  2004,  2087,  1997,  1996,  2047,  2838,  2020,  2153, 18521,\n",
      "          2013,  6207,  1012,  2065,  2017,  1000,  2128,  1000,  1037,  7473,\n",
      "          2059,  6616,  2125],\n",
      "        [ 9078,  2099,  2038,  2275,  2033,  2039,  2007,  2489,  7233, 15303,\n",
      "          1010,  2043,  2027,  2024,  2800,  2144,  1045,  2356,  1012,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[-1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1, -1,  0,  0,  0,  0,  0],\n",
      "        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1,\n",
      "          1,  1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1,  1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1,  0,  0],\n",
      "        [-1, -1, -1, -1, -1,  0,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1],\n",
      "        [-1, -1, -1, -1, -1, -1, -1, -1,  2,  2, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    a,b,c,d = batch\n",
    "    print(a)\n",
    "    print(b)\n",
    "    print(c)\n",
    "    print(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_ATE(loader, epochs):\n",
    "    all_data = len(loader)\n",
    "    for epoch in range(epochs):\n",
    "        finish_data = 0\n",
    "        losses = []\n",
    "        correct_predictions = 0\n",
    "        t0 = time.time()\n",
    "        for data in loader:\n",
    "            ids_tensors, tags_tensors, _, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(DEVICE)\n",
    "            tags_tensors = tags_tensors.to(DEVICE)\n",
    "            masks_tensors = masks_tensors.to(DEVICE)\n",
    "\n",
    "            loss = model_ATE(ids_tensors=ids_tensors, tags_tensors=tags_tensors, masks_tensors=masks_tensors)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer_ATE.step()\n",
    "            optimizer_ATE.zero_grad()\n",
    "\n",
    "        current_time = (round(time.time()-t0,3))\n",
    "        hr, min, sec = evl_time(current_time)\n",
    "        print('epoch:', epoch, \" loss:\", np.mean(losses), \" hr:\", hr, \" min:\", min,\" sec:\", sec)         \n",
    "\n",
    "        save_model(model_ATE, 'bert_ATE.pkl')\n",
    "        \n",
    "def test_model_ATE(loader):\n",
    "    pred = []\n",
    "    trueth = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "\n",
    "            ids_tensors, tags_tensors, _, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(DEVICE)\n",
    "            tags_tensors = tags_tensors.to(DEVICE)\n",
    "            masks_tensors = masks_tensors.to(DEVICE)\n",
    "\n",
    "            outputs = model_ATE(ids_tensors=ids_tensors, tags_tensors=None, masks_tensors=masks_tensors)\n",
    "\n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            pred += list([int(j) for i in predictions for j in i ])\n",
    "            trueth += list([int(j) for i in tags_tensors for j in i ])\n",
    "\n",
    "    return trueth, pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.04603916781836714  hr: 211  min: 12  sec: 58\n"
     ]
    }
   ],
   "source": [
    "%time train_model_ATE(train_loader, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ATE = load_model(model_ATE, 'bert_ATE.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.58 s, sys: 14.7 s, total: 24.3 s\n",
      "Wall time: 50.1 s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99    143669\n",
      "           1       0.87      0.88      0.87      6486\n",
      "           2       0.89      0.77      0.83      3837\n",
      "\n",
      "    accuracy                           0.98    153992\n",
      "   macro avg       0.92      0.88      0.90    153992\n",
      "weighted avg       0.98      0.98      0.98    153992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time x, y = test_model_ATE(test_loader)\n",
    "print(classification_report(x, y, target_names=[str(i) for i in range(3)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aspect Based Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops_train_ds = dataset_ABSA(pd.read_csv(\"data/laptops_train.csv\"), tokenizer)\n",
    "laptops_test_ds = dataset_ABSA(pd.read_csv(\"data/laptops_test.csv\"), tokenizer)\n",
    "restaurants_train_ds = dataset_ABSA(pd.read_csv(\"data/restaurants_train.csv\"), tokenizer)\n",
    "restaurants_test_ds = dataset_ABSA(pd.read_csv(\"data/restaurants_test.csv\"), tokenizer)\n",
    "twitter_train_ds = dataset_ABSA(pd.read_csv(\"data/twitter_train.csv\"), tokenizer)\n",
    "twitter_test_ds = dataset_ABSA(pd.read_csv(\"data/twitter_test.csv\"), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[cls]', 'the', 'battery', 'life', 'seems', 'to', 'be', 'very', 'good', ',', 'and', 'have', 'had', 'no', 'issues', 'with', 'it', '.', '[sep]', 'battery', 'life']\n",
      "21\n",
      "tensor([ 100, 1996, 6046, 2166, 3849, 2000, 2022, 2200, 2204, 1010, 1998, 2031,\n",
      "        2018, 2053, 3314, 2007, 2009, 1012,  100, 6046, 2166])\n",
      "21\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1])\n",
      "21\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "w,x,y,z = laptops_train_ds.__getitem__(121)\n",
    "print(w)\n",
    "print(len(w))\n",
    "print(x)\n",
    "print(len(x))\n",
    "print(y)\n",
    "print(len(y))\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch2(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True)\n",
    "\n",
    "    segments_tensors = [s[2] for s in samples]\n",
    "    segments_tensors = pad_sequence(segments_tensors, batch_first=True)\n",
    "\n",
    "    label_ids = torch.stack([s[3] for s in samples])\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1)\n",
    "\n",
    "    return ids_tensors, segments_tensors, masks_tensors, label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ConcatDataset([laptops_train_ds, restaurants_train_ds, twitter_train_ds])\n",
    "test_ds = ConcatDataset([laptops_test_ds, restaurants_test_ds, twitter_test_ds])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, collate_fn=create_mini_batch2, shuffle = True)\n",
    "test_loader = DataLoader(test_ds, batch_size=50, collate_fn=create_mini_batch2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  100,  2026,  2069, 10520,  2007,  1996,  2410,  1000,  1000,  2944,\n",
      "          2003,  2008,  2027,  1000,  2128,  1000,  1996,  2069,  3924,  1999,\n",
      "          1996,  6097,  8654,  4013,  2240,  2039,  2302,  2019,  1045,  2629,\n",
      "          2030,  1045,  2581, 13151,  1998,  7037,  8389,  5329,  1010,  2174,\n",
      "          1996,  3119,  1011,  2125,  2003,  2008,  2017,  6162,  1037,  2936,\n",
      "          6046,  2166,  1011,  1048, 15185,  1011,  1997,  2055,  1016,  2062,\n",
      "          2847,  1011, 25269,  2497,  1011,  1012,   100, 13151],\n",
      "        [  100,  1045,  4299,  9733,  1012,  4012,  2052,  1000, 23961,  1000,\n",
      "          1043, 10626,  8757,  9465, 10272,  1998,  4532, 28619,  2078,  1012,\n",
      "          2175,  2185,  2175,  2361,  1012,  2017,  1000,  2128,  1000,  2025,\n",
      "          2359,  1012,   100,  4532, 28619,  2078,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  100,  1996,  2069,  3471,  2024,  1996,  2614,  3475,  2102,  2200,\n",
      "          5189,  1045,  2031,  2000,  4929,  2132, 19093,  1012,   100,  2614,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  100,  5463,  2030,  6133,  1037,  3034,  1029,  2085,  1010,  8224,\n",
      "          4400,  2003,  2067, 26058,  1011,  1048, 15185,  1011,  2062, 14336,\n",
      "          2084,  1056, 28394,  3215,  1011, 25269,  2497,  1011,  2522, 25855,\n",
      "          4859,  1011,  1028, 10166,   100,  8224,  4400,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "torch.Size([4, 68])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "torch.Size([4, 68])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "torch.Size([4, 68])\n",
      "tensor([0, 1, 0, 1])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    w,x,y,z = batch\n",
    "    print(w)\n",
    "    print(w.size())\n",
    "    print(x)\n",
    "    print(x.size())\n",
    "    print(y)\n",
    "    print(y.size())\n",
    "    print(z)\n",
    "    print(z.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_ABSA(loader, epochs):\n",
    "    all_data = len(loader)\n",
    "    for epoch in range(epochs):\n",
    "        finish_data = 0\n",
    "        losses = []\n",
    "        current_times = []\n",
    "        correct_predictions = 0\n",
    "        \n",
    "        for data in loader:\n",
    "            t0 = time.time()\n",
    "            ids_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "            ids_tensors = ids_tensors.to(DEVICE)\n",
    "            segments_tensors = segments_tensors.to(DEVICE)\n",
    "            label_ids = label_ids.to(DEVICE)\n",
    "            masks_tensors = masks_tensors.to(DEVICE)\n",
    "\n",
    "            loss = model_ABSA(ids_tensors=ids_tensors, lable_tensors=label_ids, masks_tensors=masks_tensors, segments_tensors=segments_tensors)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer_ABSA.step()\n",
    "            optimizer_ABSA.zero_grad()\n",
    "\n",
    "            finish_data += 1\n",
    "            current_times.append(round(time.time()-t0,3))\n",
    "            current = np.mean(current_times)\n",
    "            hr, min, sec = evl_time(current*(all_data-finish_data) + current*all_data*(epochs-epoch-1))\n",
    "            print('epoch:', epoch, \" batch:\", finish_data, \"/\" , all_data, \" loss:\", np.mean(losses), \" hr:\", hr, \" min:\", min,\" sec:\", sec)         \n",
    "\n",
    "        save_model(model_ABSA, 'bert_ABSA2.pkl')\n",
    "        \n",
    "def test_model_ABSA(loader):\n",
    "    pred = []\n",
    "    trueth = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "\n",
    "            ids_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "            ids_tensors = ids_tensors.to(DEVICE)\n",
    "            segments_tensors = segments_tensors.to(DEVICE)\n",
    "            masks_tensors = masks_tensors.to(DEVICE)\n",
    "\n",
    "            outputs = model_ABSA(ids_tensors, None, masks_tensors=masks_tensors, segments_tensors=segments_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "\n",
    "            pred += list([int(i) for i in predictions])\n",
    "            trueth += list([int(i) for i in label_ids])\n",
    "\n",
    "    return trueth, pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m, in \u001b[0;36mtrain_model_ABSA\u001b[0;34m(loader, epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m label_ids \u001b[38;5;241m=\u001b[39m label_ids\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     15\u001b[0m masks_tensors \u001b[38;5;241m=\u001b[39m masks_tensors\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 17\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_ABSA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlable_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasks_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegments_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msegments_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Projects/ABSA/model/bert.py:37\u001b[0m, in \u001b[0;36mbert_ABSA.forward\u001b[0;34m(self, ids_tensors, lable_tensors, masks_tensors, segments_tensors)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, ids_tensors, lable_tensors, masks_tensors, segments_tensors):\n\u001b[1;32m     36\u001b[0m     bert_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(input_ids\u001b[38;5;241m=\u001b[39mids_tensors, attention_mask\u001b[38;5;241m=\u001b[39mmasks_tensors, token_type_ids\u001b[38;5;241m=\u001b[39msegments_tensors, return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mbert_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m())\n\u001b[1;32m     38\u001b[0m     linear_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(bert_outputs)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# print(linear_outputs.size())\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "%time train_model_ABSA(train_loader, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ABSA = load_model(model_ABSA, 'bert_ABSA.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time x, y = test_model_ABSA(test_loader)\n",
    "print(classification_report(x, y, target_names=[str(i) for i in range(3)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATE + ABSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model_ABSA(sentence, aspect, tokenizer):\n",
    "    t1 = tokenizer.tokenize(sentence)\n",
    "    t2 = tokenizer.tokenize(aspect)\n",
    "\n",
    "    word_pieces = ['[cls]']\n",
    "    word_pieces += t1\n",
    "    word_pieces += ['[sep]']\n",
    "    word_pieces += t2\n",
    "\n",
    "    segment_tensor = [0] + [0]*len(t1) + [0] + [1]*len(t2)\n",
    "\n",
    "    ids = tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "    input_tensor = torch.tensor([ids]).to(DEVICE)\n",
    "    segment_tensor = torch.tensor(segment_tensor).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_ABSA(input_tensor, None, None, segments_tensors=segment_tensor)\n",
    "        _, predictions = torch.max(outputs, dim=1)\n",
    "    \n",
    "    return word_pieces, predictions, outputs\n",
    "\n",
    "def predict_model_ATE(sentence, tokenizer):\n",
    "    word_pieces = []\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    word_pieces += tokens\n",
    "\n",
    "    ids = tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "    input_tensor = torch.tensor([ids]).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_ATE(input_tensor, None, None)\n",
    "        _, predictions = torch.max(outputs, dim=2)\n",
    "    predictions = predictions[0].tolist()\n",
    "\n",
    "    return word_pieces, predictions, outputs\n",
    "\n",
    "def ATE_ABSA(text):\n",
    "    terms = []\n",
    "    word = \"\"\n",
    "    x, y, z = predict_model_ATE(text, tokenizer)\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 1:\n",
    "            if len(word) != 0:\n",
    "                terms.append(word.replace(\" ##\",\"\"))\n",
    "            word = x[i]\n",
    "        if y[i] == 2:\n",
    "            word += (\" \" + x[i])\n",
    "            \n",
    "    \n",
    "    if len(word) != 0:\n",
    "            terms.append(word.replace(\" ##\",\"\"))\n",
    "            \n",
    "    print(\"tokens:\", x)\n",
    "    print(\"ATE:\", terms)\n",
    "    \n",
    "    if len(terms) != 0:\n",
    "        for i in terms:\n",
    "            _, c, p = predict_model_ABSA(text, i, tokenizer)\n",
    "            print(\"term:\", [i], \"class:\", [int(c)], \"ABSA:\", [float(p[0][0]), float(p[0][1]), float(p[0][2])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ABSA = load_model(model_ABSA, 'bert_ABSA.pkl')\n",
    "model_ATE = load_model(model_ATE, 'bert_ATE.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"For the price you pay this product is very good. However, battery life is a little lack-luster coming from a MacBook Pro.\"\n",
    "ATE_ABSA(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I think Apple is better than Microsoft.\"\n",
    "ATE_ABSA(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyberpunk 2077 - Xbox One\n",
    "\n",
    "https://www.amazon.com/-/zh_TW/Cyberpunk-2077-Xbox-One/product-reviews/B07DJW4WZC/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Spent 5 hours downloading updates.\"\n",
    "ATE_ABSA(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Install is buggy, so after downloading a day one patch that's nearly 3 times the size of the game, it glitched on the CDs and had to reinstall the game from scratch.\"\n",
    "ATE_ABSA(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Cyberpunk 2077 freezes constantly, frame rates are terrible, and it's extremely frustrating to try to play.\"\n",
    "ATE_ABSA(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Cyberpunk 2077 is completely unplayable on xbox one. They should have never released this for current gen.\"\n",
    "ATE_ABSA(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"It’s just a cash grab, the game crashes constantly, runs at like 20 fps, half the environment and characters only load when you’re three feet away from them. Unless you’re in a small space the game looks awful. The worst game i’ve ever played in years visually. It looks worse than later xbox 360 games.\"\n",
    "ATE_ABSA(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"CD Projekt Red should have just abandoned the current gen consoles instead of cheating people out of their money.\"\n",
    "ATE_ABSA(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
